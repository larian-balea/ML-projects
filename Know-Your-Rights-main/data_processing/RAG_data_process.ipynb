{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dc1c46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import fitz  \n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import faiss\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02957d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—‚ï¸  Procesez directorul: dataset\n",
      "   GÄƒsite 6 fiÈ™iere PDF\n",
      "ğŸ” Procesez: COD FISCAL (A) 08_09_2015.pdf\n",
      "   Lungime text brut: 2448117 caractere\n",
      "   Lungime text curat: 2415628 caractere\n",
      "   Tip document: CF\n",
      "ğŸ” DEBUG pentru CF:\n",
      "   Total articole gÄƒsite cu ARTICLE_RE: 2231\n",
      "   Primele 10: [('1', 'Scopul'), ('2', 'Impozitele,'), ('2', ','), ('2', ','), ('2', ','), ('3', 'Principiile'), ('4', 'Modificarea'), ('4', ','), ('5', 'Norme'), ('6', 'Aplicarea')]\n",
      "   ReferinÈ›e gÄƒsite (de exclus): 2539\n",
      "   Primele referinÈ›e: ['prevederile art. 157', 'prevederile art. 41', 'la art. 16', 'la art. 2', 'din Articolul 2']\n",
      "   ğŸ¯ FINAL: 1559 articole extrase din CF\n",
      "   âœ… 1559 articole extrase\n",
      "ğŸ” Procesez: codul-muncii.pdf\n",
      "   Lungime text brut: 220710 caractere\n",
      "   Lungime text curat: 218786 caractere\n",
      "   Tip document: CM\n",
      "ğŸ” DEBUG pentru CM:\n",
      "   Total articole gÄƒsite cu ARTICLE_RE: 293\n",
      "   Primele 10: [('1', '(1)'), ('2', 'Dispozitiile'), ('3', '(1)'), ('4', '(1)'), ('5', '(1)'), ('6', '(1)'), ('7', 'Salariatii'), ('8', '(1)'), ('9', 'Cetatenii'), ('10', 'Contractul')]\n",
      "   ReferinÈ›e gÄƒsite (de exclus): 80\n",
      "   Primele referinÈ›e: ['prevederile art. 39', 'la art. 39', 'la art. 105', 'la art. 17', 'la art. 17']\n",
      "   ğŸ¯ FINAL: 263 articole extrase din CM\n",
      "   âœ… 263 articole extrase\n",
      "ğŸ” Procesez: CONSTITUTIA ROMÃ‚NIEI(1).pdf\n",
      "   Lungime text brut: 5099 caractere\n",
      "   Lungime text curat: 4918 caractere\n",
      "   Tip document: CONST\n",
      "ğŸ” DEBUG pentru CONST:\n",
      "   Total articole gÄƒsite cu ARTICLE_RE: 14\n",
      "   Primele 10: [('1', '(1)'), ('2', '(1)'), ('3', '(1)'), ('4', '(1)'), ('5', '(1)'), ('6', '(1)'), ('7', 'Statul'), ('8', '(1)'), ('9', 'Sindicatele,'), ('10', 'RomÃ¢nia')]\n",
      "   ReferinÈ›e gÄƒsite (de exclus): 1\n",
      "   Primele referinÈ›e: ['la\\nARTICOLUL 14']\n",
      "   ğŸ¯ FINAL: 13 articole extrase din CONST\n",
      "   âœ… 13 articole extrase\n",
      "ğŸ” Procesez: CONSTITUTIA ROMÃ‚NIEI(2).pdf\n",
      "   Lungime text brut: 25655 caractere\n",
      "   Lungime text curat: 24870 caractere\n",
      "   Tip document: CONST\n",
      "ğŸ” DEBUG pentru CONST:\n",
      "   Total articole gÄƒsite cu ARTICLE_RE: 46\n",
      "   Primele 10: [('15', '(1)'), ('16', '(1)'), ('17', 'CetÄƒÅ£enii'), ('18', '(1)'), ('19', '(1)'), ('20', '(1)'), ('21', '(1)'), ('22', '(1)'), ('23', '(1)'), ('24', '(1)')]\n",
      "   ReferinÈ›e gÄƒsite (de exclus): 0\n",
      "   ğŸ¯ FINAL: 46 articole extrase din CONST\n",
      "   âœ… 46 articole extrase\n",
      "ğŸ” Procesez: CONSTITUTIA ROMÃ‚NIEI(3).pdf\n",
      "   Lungime text brut: 44935 caractere\n",
      "   Lungime text curat: 43597 caractere\n",
      "   Tip document: CONST\n",
      "ğŸ” DEBUG pentru CONST:\n",
      "   Total articole gÄƒsite cu ARTICLE_RE: 74\n",
      "   Primele 10: [('61', '(1)'), ('62', '(1)'), ('63', '(1)'), ('64', '(1)'), ('65', '(1)'), ('66', '(1)'), ('67', 'Camera'), ('68', '(1)'), ('69', '(1)'), ('70', '(1)')]\n",
      "   ReferinÈ›e gÄƒsite (de exclus): 6\n",
      "   Primele referinÈ›e: ['la\\narticolul 31', 'la articolul 82', 'la articolul 106', 'la articolul 106', 'la\\narticolul 76']\n",
      "   ğŸ¯ FINAL: 73 articole extrase din CONST\n",
      "   âœ… 73 articole extrase\n",
      "ğŸ” Procesez: noul-cod-civil.pdf\n",
      "   Lungime text brut: 1281095 caractere\n",
      "   Lungime text curat: 1244878 caractere\n",
      "   Tip document: CC\n",
      "ğŸ” DEBUG pentru CC:\n",
      "   Total articole gÄƒsite cu ARTICLE_RE: 2651\n",
      "   Primele 10: [('1', '.'), ('2', '.'), ('3', '.'), ('4', '.'), ('5', '.'), ('6', '.'), ('7', '.'), ('8', '.'), ('9', '.'), ('10', '.')]\n",
      "   ReferinÈ›e gÄƒsite (de exclus): 97\n",
      "   Primele referinÈ›e: ['la art. 41', 'la  art. 43', 'la  art. 252', 'la art. 75', 'la art. 75']\n",
      "   ğŸ¯ FINAL: 2604 articole extrase din CC\n",
      "   âœ… 2604 articole extrase\n",
      "\n",
      "âœ… Total chunks create: 4558\n",
      "\n",
      "ğŸ“Š Statistici chunks:\n",
      "Total chunks: 4558\n",
      "CC: 2604 articole\n",
      "CF: 1559 articole\n",
      "CM: 263 articole\n",
      "CONST: 132 articole\n",
      "\n",
      "âŒ AnalizÄƒ fiÈ™iere:\n",
      "   âœ… COD FISCAL (A) 08_09_2015.pdf (tip: CF) - 1559 articole\n",
      "   âœ… codul-muncii.pdf (tip: CM) - 263 articole\n",
      "   âœ… CONSTITUTIA ROMÃ‚NIEI(1).pdf (tip: CONST) - 132 articole\n",
      "   âœ… CONSTITUTIA ROMÃ‚NIEI(2).pdf (tip: CONST) - 132 articole\n",
      "   âœ… CONSTITUTIA ROMÃ‚NIEI(3).pdf (tip: CONST) - 132 articole\n",
      "   âœ… noul-cod-civil.pdf (tip: CC) - 2604 articole\n",
      "âœ… Toate fiÈ™ierele au fost procesate cu succes!\n",
      "ğŸ”„ Configurare model embeddings...\n",
      "âœ… Model configurat dimensiune 768\n",
      "ğŸ”„ Generez embeddings pentru 4558 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [10:48<00:00,  4.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 4558 embeddings generate\n",
      "âœ… Index FAISS creat cu 4558 vectori de dimensiune 768\n",
      "ğŸ’¾ Vector DB assets salvate Ã®n processed:\n",
      "   ğŸ“ embeddings_with_metadata.json - pentru vector DB\n",
      "   ğŸ“ faiss_index.bin - pentru cÄƒutare\n",
      "   ğŸ“ embeddings.npy - array numpy\n",
      "   ğŸ“ chunks_text.json - texte pentru debug\n",
      "\n",
      "ğŸ” Demo cÄƒutare:\n",
      "\n",
      "â“ Ãntrebare: 'contractul de muncÄƒ'\n",
      "   1. [CM_art_37] Score: 0.574\n",
      "      si obligatiile privind relatiile de munca dintre angajator si salariat se stabilesc potrivit legii, ...\n",
      "   2. [CM_art_21] Score: 0.536\n",
      "      La incheierea contractului individual de munca sau pe parcursul executarii acestuia, partile pot neg...\n",
      "   3. [CM_art_16] Score: 0.525\n",
      "      Contractul individual de munca se incheie in baza consimtamantului partilor, in forma scrisa, in lim...\n",
      "\n",
      "â“ Ãntrebare: 'drepturile salariatului'\n",
      "   1. [CM_art_39] Score: 0.738\n",
      "      Salariatul are, in principal, urmatoarele drepturi: \n",
      "a) dreptul la salarizare pentru munca depusa; \n",
      "...\n",
      "   2. [CM_art_47] Score: 0.714\n",
      "      Drepturile cuvenite salariatului detasat se acorda de angajatorul la care s-a dispus detasarea. \n",
      "(2)...\n",
      "   3. [CONST_art_43] Score: 0.615\n",
      "      SalariaÅ£ii au dreptul la grevÄƒ pentru apÄƒrarea intereselor profesionale, economice\n",
      "ÅŸi sociale.\n",
      "(2) L...\n",
      "\n",
      "â“ Ãntrebare: 'concediul de odihnÄƒ'\n",
      "   1. [CM_art_133] Score: 0.719\n",
      "      de repaus reprezinta orice perioada care nu este timp de munca. \n",
      "SECTIUNEA 1 \n",
      "Pauza de masa si repau...\n",
      "   2. [CM_art_151] Score: 0.360\n",
      "      Concediul de odihna poate fi intrerupt, la cererea salariatului, pentru motive obiective. \n",
      "(2) Angaj...\n",
      "   3. [CM_art_149] Score: 0.314\n",
      "      este obligat sa efectueze in natura concediul de odihna in perioada in care a fost programat, cu exc...\n",
      "\n",
      "â“ Ãntrebare: 'salariul minim'\n",
      "   1. [CM_art_164] Score: 0.538\n",
      "      Salariul de baza minim brut pe tara garantat in plata, corespunzator programului normal de munca, se...\n",
      "   2. [CM_art_165] Score: 0.466\n",
      "      salariatii carora angajatorul, conform contractului colectiv sau individual de munca, le asigura hra...\n",
      "   3. [CM_art_103] Score: 0.415\n",
      "      cu fractiune de norma este salariatul al carui numar de ore normale de lucru, calculate saptamanal s...\n",
      "\n",
      "â“ Ãntrebare: 'Ã®ntreruperea contractului de muncÄƒ'\n",
      "   1. [CM_art_21] Score: 0.513\n",
      "      La incheierea contractului individual de munca sau pe parcursul executarii acestuia, partile pot neg...\n",
      "   2. [CM_art_56] Score: 0.510\n",
      "      Contractul individual de munca existent inceteaza de drept: \n",
      "a) la data decesului salariatului sau a...\n",
      "   3. [CM_art_22] Score: 0.458\n",
      "      Clauza de neconcurenta isi poate produce efectele pentru o perioada de maximum 2 ani de la data ince...\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'Pagina?\\s+\\d+\\s+din\\s+\\d+', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\d+/\\d+', '', text)\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    text = re.sub(r'\\d{2}\\.\\d{2}\\.\\d{4}', '', text)\n",
    "    \n",
    "    # NormalizeazÄƒ spaÈ›iile\n",
    "    text = re.sub(r'\\s{3,}', ' ', text)\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def identify_doc_type(filename):\n",
    "    filename = filename.lower()\n",
    "    if 'codul-muncii' in filename or 'cod muncii' in filename:\n",
    "        return 'CM'\n",
    "    elif 'constitutia' in filename:\n",
    "        return 'CONST'\n",
    "    elif 'cod-civil' in filename or 'cod civil' in filename:\n",
    "        return 'CC'\n",
    "    elif 'cod-fiscal' in filename or 'cod fiscal' in filename:\n",
    "        return 'CF'\n",
    "    else:\n",
    "        return 'LEGAL'\n",
    "\n",
    "def is_article_reference(text_line):\n",
    "\n",
    "    reference_patterns = [\n",
    "        r'^\\s*(?:conform|potrivit|prevederile|prevazut\\s+(?:la|de)|dispozitiile|in\\s+sensul|in\\s+conditiile)\\s+(?:art\\.|articolul)\\s+\\d+',\n",
    "        r'(?:conform|potrivit|prevederile|prevazut\\s+(?:la|de)|dispozitiile)\\s+(?:art\\.|articolul)\\s+\\d+(?:\\s*[-,.]|\\s*$)',\n",
    "        r'^\\s*(?:art\\.|articolul)\\s+\\d+\\s*[-,.]?\\s*(?:alin|lit|pct)\\.',  # art. 132 alin. 1\n",
    "        r'la\\s+(?:art\\.|articolul)\\s+\\d+',  # la art. 132\n",
    "        r'din\\s+(?:art\\.|articolul)\\s+\\d+', # din art. 132\n",
    "        r'de\\s+(?:art\\.|articolul)\\s+\\d+'\n",
    "    ]\n",
    "\n",
    "    text_lower = text_line.lower()\n",
    "    for pattern in reference_patterns:\n",
    "        if re.search(pattern, text_lower):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def extract_articles(text, doc_type):\n",
    "    print(f\"ğŸ” DEBUG pentru {doc_type}:\")\n",
    "    chunks = []\n",
    "    \n",
    "    ARTICLE_RE = re.compile(\n",
    "        r'(?:^|\\s)(?:ARTICOLUL|Articolul|ART\\.?|Art\\.)\\s+(\\d+(?:\\^\\d+)?)\\.?\\b'\n",
    "        r'(?:\\s*[-â€“]\\s*)?(?:\\s*\\n?\\s*(.+?))?(?=\\s|$|\\n)',\n",
    "        flags=re.MULTILINE\n",
    "    )\n",
    "    \n",
    "    ARTICLE_REFERENCE = re.compile(\n",
    "        r'(?:conform|potrivit|prevederile|prevazut\\s+(?:la|de)|dispozitiile|la|din)\\s+(?:art\\.|articolul)\\s+\\d+\\b',\n",
    "        flags=re.MULTILINE | re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    all_matches = ARTICLE_RE.findall(text)\n",
    "    print(f\"   Total articole gÄƒsite cu ARTICLE_RE: {len(all_matches)}\")\n",
    "    if all_matches:\n",
    "        print(f\"   Primele 10: {all_matches[:10]}\")\n",
    "    \n",
    "    # GaseÈ™te referinÈ›ele pentru a le exclude ulterior\n",
    "    references = ARTICLE_REFERENCE.findall(text)\n",
    "    print(f\"   ReferinÈ›e gÄƒsite (de exclus): {len(references)}\")\n",
    "    if references:\n",
    "        print(f\"   Primele referinÈ›e: {references[:5]}\")\n",
    "    \n",
    "    # ItereazÄƒ prin toate meciurile gÄƒsite\n",
    "    for match in ARTICLE_RE.finditer(text):\n",
    "        full_match = match.group(0)\n",
    "        article_num = match.group(1)\n",
    "        article_title = match.group(2) if match.group(2) else \"\"\n",
    "        \n",
    "        #print(f\"   ğŸ” GÄƒsit: '{full_match.strip()}'\")\n",
    "        \n",
    "        # Verificare mai riguroasÄƒ pentru referinÈ›e\n",
    "        context_start = max(0, match.start() - 100)  # Context mai mare\n",
    "        context_end = min(len(text), match.end() + 50)\n",
    "        context = text[context_start:context_end]\n",
    "        \n",
    "        # VerificÄƒm dacÄƒ articolul este Ã®ntr-un context de referinÈ›Äƒ\n",
    "        is_reference = False\n",
    "        \n",
    "        # 1. VerificÄƒm contextul din jurul match-ului\n",
    "        if ARTICLE_REFERENCE.search(context):\n",
    "            is_reference = True\n",
    "            \n",
    "        # 2. VerificÄƒm dacÄƒ linia Ã®ntreagÄƒ pare a fi o referinÈ›Äƒ\n",
    "        line_start = text.rfind('\\n', 0, match.start()) + 1\n",
    "        line_end = text.find('\\n', match.end())\n",
    "        if line_end == -1:\n",
    "            line_end = len(text)\n",
    "        full_line = text[line_start:line_end]\n",
    "        \n",
    "        if is_article_reference(full_line):\n",
    "            is_reference = True\n",
    "            \n",
    "        # 3. VerificÄƒm dacÄƒ nu urmeazÄƒ conÈ›inut substanÈ›ial\n",
    "        content_preview = text[match.end():match.end()+10].strip()\n",
    "        if len(content_preview) < 10 and re.search(r'(?:art\\.|articolul)\\s+\\d+', content_preview, re.IGNORECASE):\n",
    "            is_reference = True\n",
    "        \n",
    "        if not is_reference:\n",
    "            # Extragem conÈ›inutul articolului\n",
    "            start_pos = match.end()\n",
    "            \n",
    "            # GÄƒsim sfÃ¢rÈ™itul articolului (urmÄƒtorul articol sau sfÃ¢rÈ™itul textului)\n",
    "            next_article = ARTICLE_RE.search(text, start_pos)\n",
    "            if next_article:\n",
    "                end_pos = next_article.start()\n",
    "                content = text[start_pos:end_pos].strip()\n",
    "            else:\n",
    "                content = text[start_pos:].strip()\n",
    "            \n",
    "            # CurÄƒÈ›Äƒm conÈ›inutul de linii goale multiple\n",
    "            content = re.sub(r'\\n{3,}', '\\n\\n', content)\n",
    "            content = content.strip()\n",
    "            \n",
    "            if content and len(content) > 10:  # Doar dacÄƒ avem conÈ›inut substanÈ›ial\n",
    "                chunk = {\n",
    "                    'id': f\"{doc_type}_art_{article_num}\",\n",
    "                    'text': content,\n",
    "                    'metadata': {\n",
    "                        'doc_type': doc_type,\n",
    "                        'article_number': article_num,\n",
    "                        'article_title': article_title.strip() if article_title else \"\"\n",
    "                    }\n",
    "                }\n",
    "                chunks.append(chunk)\n",
    "                #print(f\"   âœ… Articol {article_num} extras ({len(content)} caractere)\")\n",
    "            else:\n",
    "                print(f\"   âš ï¸  Articol {article_num} - conÈ›inut prea scurt: '{content[:50]}'\")\n",
    "        #else:\n",
    "            #print(f\"   âœ— Exclus ca referinÈ›Äƒ: 'Art. {article_num}'\")\n",
    "            #print(f\"      Context: '{context[:20]}...'\")\n",
    "            #print(f\"      Linie completÄƒ: '{full_line[:20]}...'\")\n",
    "    \n",
    "    print(f\"   ğŸ¯ FINAL: {len(chunks)} articole extrase din {doc_type}\")\n",
    "    return chunks\n",
    "\n",
    "def process_pdf(pdf_path):\n",
    "\n",
    "    print(f\"ğŸ” Procesez: {pdf_path.name}\")\n",
    "    \n",
    "    raw_text = extract_text_from_pdf(pdf_path)\n",
    "    print(f\"   Lungime text brut: {len(raw_text)} caractere\")\n",
    "    \n",
    "    clean_text_content = clean_text(raw_text)\n",
    "    print(f\"   Lungime text curat: {len(clean_text_content)} caractere\")\n",
    "    \n",
    "    doc_type = identify_doc_type(pdf_path.name)\n",
    "    print(f\"   Tip document: {doc_type}\")\n",
    "    \n",
    "    chunks = extract_articles(clean_text_content, doc_type)\n",
    "    print(f\"   âœ… {len(chunks)} articole extrase\")\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def setup_embeddings():\n",
    "    print(\"ğŸ”„ Configurare model embeddings...\")\n",
    "\n",
    "    model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "    test_embedding = model.encode([\"test\"], convert_to_numpy=True)\n",
    "    print(f\"âœ… Model configurat dimensiune {test_embedding.shape[1]}\")\n",
    "    return model\n",
    "\n",
    "def generate_embeddings(chunks, model, batch_size=32):    \n",
    "    print(f\"ğŸ”„ Generez embeddings pentru {len(chunks)} chunks...\")\n",
    "\n",
    "    texts = [chunk['text'] for chunk in chunks]\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(chunks), batch_size)):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        batch_embeddings = model.encode(batch, convert_to_numpy=True, show_progress_bar=False)\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    for chunk, embedding in zip(chunks, embeddings):\n",
    "        chunk['embedding'] = embedding\n",
    "    \n",
    "    print(f\"âœ… {len(embeddings)} embeddings generate\")\n",
    "    return chunks\n",
    "\n",
    "def create_faiss_index(chunks_with_embeddings):\n",
    "\n",
    "    embeddings = np.array([chunk['embedding'] for chunk in chunks_with_embeddings])\n",
    "    dimension = embeddings.shape[1]\n",
    "    \n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    print(f\"âœ… Index FAISS creat cu {index.ntotal} vectori de dimensiune {dimension}\")\n",
    "    return index\n",
    "\n",
    "def save_processed_data(chunks, index, model, output_dir=\"processed\"):\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # 1.  Save embeddings + metadata pentru vector DB\n",
    "    embeddings_data = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        embeddings_data.append({\n",
    "            'id': chunk['id'],\n",
    "            'embedding': chunk['embedding'].tolist(),  # Convert numpy to list for JSON\n",
    "            'metadata': chunk['metadata']\n",
    "        })\n",
    "    \n",
    "    with open(output_path / \"embeddings_with_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(embeddings_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Debbug: SalveazÄƒ textul brut al chunk-urilor\n",
    "    texts_data = []\n",
    "    for chunk in chunks:\n",
    "        texts_data.append({\n",
    "            'id': chunk['id'],\n",
    "            'text': chunk['text'],\n",
    "            'metadata': chunk['metadata']\n",
    "        })\n",
    "    \n",
    "    with open(output_path / \"chunks_text.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(texts_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # 3. Salvam index FAISS - necesar pentru cÄƒutare rapidÄƒ\n",
    "    faiss.write_index(index, str(output_path / \"faiss_index.bin\"))\n",
    "    \n",
    "    # 4. Salvam embeddings ca numpy array - loading rapid \n",
    "    embeddings_array = np.array([chunk['embedding'] for chunk in chunks])\n",
    "    np.save(output_path / \"embeddings.npy\", embeddings_array)\n",
    "    \n",
    "    print(f\"ğŸ’¾ Vector DB assets salvate Ã®n {output_dir}:\")\n",
    "    print(f\"   ğŸ“ embeddings_with_metadata.json - pentru vector DB\")\n",
    "    print(f\"   ğŸ“ faiss_index.bin - pentru cÄƒutare\")\n",
    "    print(f\"   ğŸ“ embeddings.npy - array numpy\")\n",
    "    print(f\"   ğŸ“ chunks_text.json - texte pentru debug\")\n",
    "\n",
    "def load_processed_data(input_dir=\"processed\"):\n",
    "\n",
    "    input_path = Path(input_dir)\n",
    "    \n",
    "    with open(input_path / \"embeddings_with_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        embeddings_data = json.load(f)\n",
    "    \n",
    "    index = faiss.read_index(str(input_path / \"faiss_index.bin\"))\n",
    "    \n",
    "    with open(input_path / \"chunks_text.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        texts_data = json.load(f)\n",
    "    \n",
    "    print(f\"âœ… Date Ã®ncÄƒrcate din {input_dir}\")\n",
    "    print(f\"   ğŸ“Š {len(embeddings_data)} embeddings\")\n",
    "    print(f\"   ğŸ” Index FAISS cu {index.ntotal} vectori\")\n",
    "    \n",
    "    return embeddings_data, index, texts_data\n",
    "\n",
    "def show_statistics(chunks):\n",
    "    print(\"\\nğŸ“Š Statistici chunks:\")\n",
    "    print(f\"Total chunks: {len(chunks)}\")\n",
    "    \n",
    "    doc_stats = {}\n",
    "    for chunk in chunks:\n",
    "        doc_type = chunk['metadata']['doc_type']\n",
    "        doc_stats[doc_type] = doc_stats.get(doc_type, 0) + 1\n",
    "    \n",
    "    for doc_type, count in sorted(doc_stats.items()):\n",
    "        print(f\"{doc_type}: {count} articole\")\n",
    "\n",
    "def show_failed_extractions(directory_path, chunks):\n",
    "\n",
    "    directory = Path(directory_path)\n",
    "    pdf_files = list(directory.glob(\"*.pdf\"))\n",
    "    \n",
    "    # ColecteazÄƒ tipurile de documente care au avut extracÈ›ii\n",
    "    extracted_files = {}\n",
    "    for chunk in chunks:\n",
    "        doc_type = chunk['metadata']['doc_type']\n",
    "        if doc_type not in extracted_files:\n",
    "            extracted_files[doc_type] = []\n",
    "        extracted_files[doc_type].append(chunk)\n",
    "    \n",
    "    failed_files = []\n",
    "    \n",
    "    print(\"\\nâŒ AnalizÄƒ fiÈ™iere:\")\n",
    "    for pdf_file in pdf_files:\n",
    "        doc_type = identify_doc_type(pdf_file.name)\n",
    "        chunk_count = len(extracted_files.get(doc_type, []))\n",
    "        \n",
    "        if chunk_count == 0:\n",
    "            failed_files.append(pdf_file)\n",
    "            print(f\"   âŒ {pdf_file.name} (tip: {doc_type}) - 0 articole\")\n",
    "        else:\n",
    "            print(f\"   âœ… {pdf_file.name} (tip: {doc_type}) - {chunk_count} articole\")\n",
    "    \n",
    "    # Pentru fiÈ™ierele failed, aratÄƒ formatul textului\n",
    "    if failed_files:\n",
    "        print(f\"\\nğŸ” ANALIZA FORMATELOR pentru {len(failed_files)} fiÈ™iere failed:\")\n",
    "        \n",
    "        for pdf_file in failed_files:\n",
    "            print(f\"\\nğŸ“„ === {pdf_file.name} ===\")\n",
    "            try:\n",
    "                raw_text = extract_text_from_pdf(pdf_file)\n",
    "                clean_text_content = clean_text(raw_text)\n",
    "                                \n",
    "                # CautÄƒ toate pattern-urile posibile\n",
    "                patterns_to_check = [\n",
    "                    (r'ARTICOLUL\\s+\\d+', 'ARTICOLUL num'),\n",
    "                    (r'Articolul\\s+\\d+', 'Articolul num'),\n",
    "                    (r'Art\\.\\s+\\d+', 'Art. num'),\n",
    "                    (r'ART\\.\\s+\\d+', 'ART. num'),\n",
    "                    (r'art\\.\\s+\\d+', 'art. num'),\n",
    "                    (r'Art\\s+\\d+', 'Art num (fÄƒrÄƒ punct)'),\n",
    "                    (r'ART\\s+\\d+', 'ART num (fÄƒrÄƒ punct)'),\n",
    "                    (r'Article\\s+\\d+', 'Article num (engleza)'),\n",
    "                    (r'\\d+\\.\\s+', 'num. (format simplu)'),\n",
    "                ]\n",
    "                \n",
    "                print(f\"ğŸ” Pattern-uri gÄƒsite:\")\n",
    "                found_any = False\n",
    "                for pattern, description in patterns_to_check:\n",
    "                    matches = re.findall(pattern, clean_text_content, re.MULTILINE | re.IGNORECASE)\n",
    "                    if matches:\n",
    "                        print(f\"   âœ“ {description}: {len(matches)} gÄƒsite - {matches[:5]}\")\n",
    "                        found_any = True\n",
    "                \n",
    "                if not found_any:\n",
    "                    print(\"   âŒ Niciun pattern de articol gÄƒsit!\")\n",
    "                \n",
    "                # CautÄƒ linii care Ã®ncep cu numere\n",
    "                lines_with_numbers = []\n",
    "                for line_num, line in enumerate(clean_text_content.split('\\n')[:100]):\n",
    "                    if re.match(r'^\\s*\\d+', line.strip()):\n",
    "                        lines_with_numbers.append(f\"Linia {line_num}: '{line.strip()}'\")\n",
    "                \n",
    "                if lines_with_numbers:\n",
    "                    print(f\"ğŸ“Š Primele linii care Ã®ncep cu numere:\")\n",
    "                    for line_info in lines_with_numbers[:10]:\n",
    "                        print(f\"   {line_info}\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ Eroare la procesarea {pdf_file.name}: {e}\")\n",
    "    else:\n",
    "        print(\"âœ… Toate fiÈ™ierele au fost procesate cu succes!\")\n",
    "\n",
    "def search_chunks(query, chunks, index, model, top_k=5):\n",
    "    \"\"\"CautÄƒ Ã®n chunks folosind similaritatea semanticÄƒ\"\"\"\n",
    "\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    results = []\n",
    "    for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "        if idx < len(chunks):\n",
    "            results.append({\n",
    "                'rank': i + 1,\n",
    "                'score': 1 - distance,  \n",
    "                'chunk': chunks[idx]\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def demo_search(chunks, index, model):\n",
    "\n",
    "    test_queries = [\n",
    "        \"contractul de muncÄƒ\",\n",
    "        \"drepturile salariatului\",\n",
    "        \"concediul de odihnÄƒ\",\n",
    "        \"salariul minim\",\n",
    "        \"Ã®ntreruperea contractului de muncÄƒ\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nğŸ” Demo cÄƒutare:\")\n",
    "    for query in test_queries:\n",
    "        print(f\"\\nâ“ Ãntrebare: '{query}'\")\n",
    "        results = search_chunks(query, chunks, index, model, top_k=3)\n",
    "        \n",
    "        for result in results:\n",
    "            chunk = result['chunk']\n",
    "            print(f\"   {result['rank']}. [{chunk['id']}] Score: {result['score']:.3f}\")\n",
    "            print(f\"      {chunk['text'][:100]}...\")\n",
    "\n",
    "def process_directory(directory_path):\n",
    "    print(f\"ğŸ—‚ï¸  Procesez directorul: {directory_path}\")\n",
    "\n",
    "    directory = Path(directory_path)\n",
    "    pdf_files = list(directory.glob(\"*.pdf\"))\n",
    "    all_chunks = []\n",
    "\n",
    "    print(f\"   GÄƒsite {len(pdf_files)} fiÈ™iere PDF\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        chunks = process_pdf(pdf_file)\n",
    "        all_chunks.extend(chunks)\n",
    "    \n",
    "    print(f\"\\nâœ… Total chunks create: {len(all_chunks)}\")\n",
    "    return all_chunks\n",
    "\n",
    "def main():\n",
    "    directory_path = \"dataset\"\n",
    "    \n",
    "    chunks = process_directory(directory_path)\n",
    "    show_statistics(chunks)\n",
    "    show_failed_extractions(directory_path, chunks)\n",
    "    \n",
    "    model = setup_embeddings()    \n",
    "    chunks_with_embeddings = generate_embeddings(chunks, model)\n",
    "    index = create_faiss_index(chunks_with_embeddings)\n",
    "    \n",
    "    save_processed_data(chunks_with_embeddings, index, model)\n",
    "    demo_search(chunks_with_embeddings, index, model)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
