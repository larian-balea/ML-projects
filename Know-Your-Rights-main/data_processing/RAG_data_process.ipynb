{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dc1c46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import fitz  \n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import faiss\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02957d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗂️  Procesez directorul: dataset\n",
      "   Găsite 6 fișiere PDF\n",
      "🔍 Procesez: COD FISCAL (A) 08_09_2015.pdf\n",
      "   Lungime text brut: 2448117 caractere\n",
      "   Lungime text curat: 2415628 caractere\n",
      "   Tip document: CF\n",
      "🔍 DEBUG pentru CF:\n",
      "   Total articole găsite cu ARTICLE_RE: 2231\n",
      "   Primele 10: [('1', 'Scopul'), ('2', 'Impozitele,'), ('2', ','), ('2', ','), ('2', ','), ('3', 'Principiile'), ('4', 'Modificarea'), ('4', ','), ('5', 'Norme'), ('6', 'Aplicarea')]\n",
      "   Referințe găsite (de exclus): 2539\n",
      "   Primele referințe: ['prevederile art. 157', 'prevederile art. 41', 'la art. 16', 'la art. 2', 'din Articolul 2']\n",
      "   🎯 FINAL: 1559 articole extrase din CF\n",
      "   ✅ 1559 articole extrase\n",
      "🔍 Procesez: codul-muncii.pdf\n",
      "   Lungime text brut: 220710 caractere\n",
      "   Lungime text curat: 218786 caractere\n",
      "   Tip document: CM\n",
      "🔍 DEBUG pentru CM:\n",
      "   Total articole găsite cu ARTICLE_RE: 293\n",
      "   Primele 10: [('1', '(1)'), ('2', 'Dispozitiile'), ('3', '(1)'), ('4', '(1)'), ('5', '(1)'), ('6', '(1)'), ('7', 'Salariatii'), ('8', '(1)'), ('9', 'Cetatenii'), ('10', 'Contractul')]\n",
      "   Referințe găsite (de exclus): 80\n",
      "   Primele referințe: ['prevederile art. 39', 'la art. 39', 'la art. 105', 'la art. 17', 'la art. 17']\n",
      "   🎯 FINAL: 263 articole extrase din CM\n",
      "   ✅ 263 articole extrase\n",
      "🔍 Procesez: CONSTITUTIA ROMÂNIEI(1).pdf\n",
      "   Lungime text brut: 5099 caractere\n",
      "   Lungime text curat: 4918 caractere\n",
      "   Tip document: CONST\n",
      "🔍 DEBUG pentru CONST:\n",
      "   Total articole găsite cu ARTICLE_RE: 14\n",
      "   Primele 10: [('1', '(1)'), ('2', '(1)'), ('3', '(1)'), ('4', '(1)'), ('5', '(1)'), ('6', '(1)'), ('7', 'Statul'), ('8', '(1)'), ('9', 'Sindicatele,'), ('10', 'România')]\n",
      "   Referințe găsite (de exclus): 1\n",
      "   Primele referințe: ['la\\nARTICOLUL 14']\n",
      "   🎯 FINAL: 13 articole extrase din CONST\n",
      "   ✅ 13 articole extrase\n",
      "🔍 Procesez: CONSTITUTIA ROMÂNIEI(2).pdf\n",
      "   Lungime text brut: 25655 caractere\n",
      "   Lungime text curat: 24870 caractere\n",
      "   Tip document: CONST\n",
      "🔍 DEBUG pentru CONST:\n",
      "   Total articole găsite cu ARTICLE_RE: 46\n",
      "   Primele 10: [('15', '(1)'), ('16', '(1)'), ('17', 'Cetăţenii'), ('18', '(1)'), ('19', '(1)'), ('20', '(1)'), ('21', '(1)'), ('22', '(1)'), ('23', '(1)'), ('24', '(1)')]\n",
      "   Referințe găsite (de exclus): 0\n",
      "   🎯 FINAL: 46 articole extrase din CONST\n",
      "   ✅ 46 articole extrase\n",
      "🔍 Procesez: CONSTITUTIA ROMÂNIEI(3).pdf\n",
      "   Lungime text brut: 44935 caractere\n",
      "   Lungime text curat: 43597 caractere\n",
      "   Tip document: CONST\n",
      "🔍 DEBUG pentru CONST:\n",
      "   Total articole găsite cu ARTICLE_RE: 74\n",
      "   Primele 10: [('61', '(1)'), ('62', '(1)'), ('63', '(1)'), ('64', '(1)'), ('65', '(1)'), ('66', '(1)'), ('67', 'Camera'), ('68', '(1)'), ('69', '(1)'), ('70', '(1)')]\n",
      "   Referințe găsite (de exclus): 6\n",
      "   Primele referințe: ['la\\narticolul 31', 'la articolul 82', 'la articolul 106', 'la articolul 106', 'la\\narticolul 76']\n",
      "   🎯 FINAL: 73 articole extrase din CONST\n",
      "   ✅ 73 articole extrase\n",
      "🔍 Procesez: noul-cod-civil.pdf\n",
      "   Lungime text brut: 1281095 caractere\n",
      "   Lungime text curat: 1244878 caractere\n",
      "   Tip document: CC\n",
      "🔍 DEBUG pentru CC:\n",
      "   Total articole găsite cu ARTICLE_RE: 2651\n",
      "   Primele 10: [('1', '.'), ('2', '.'), ('3', '.'), ('4', '.'), ('5', '.'), ('6', '.'), ('7', '.'), ('8', '.'), ('9', '.'), ('10', '.')]\n",
      "   Referințe găsite (de exclus): 97\n",
      "   Primele referințe: ['la art. 41', 'la  art. 43', 'la  art. 252', 'la art. 75', 'la art. 75']\n",
      "   🎯 FINAL: 2604 articole extrase din CC\n",
      "   ✅ 2604 articole extrase\n",
      "\n",
      "✅ Total chunks create: 4558\n",
      "\n",
      "📊 Statistici chunks:\n",
      "Total chunks: 4558\n",
      "CC: 2604 articole\n",
      "CF: 1559 articole\n",
      "CM: 263 articole\n",
      "CONST: 132 articole\n",
      "\n",
      "❌ Analiză fișiere:\n",
      "   ✅ COD FISCAL (A) 08_09_2015.pdf (tip: CF) - 1559 articole\n",
      "   ✅ codul-muncii.pdf (tip: CM) - 263 articole\n",
      "   ✅ CONSTITUTIA ROMÂNIEI(1).pdf (tip: CONST) - 132 articole\n",
      "   ✅ CONSTITUTIA ROMÂNIEI(2).pdf (tip: CONST) - 132 articole\n",
      "   ✅ CONSTITUTIA ROMÂNIEI(3).pdf (tip: CONST) - 132 articole\n",
      "   ✅ noul-cod-civil.pdf (tip: CC) - 2604 articole\n",
      "✅ Toate fișierele au fost procesate cu succes!\n",
      "🔄 Configurare model embeddings...\n",
      "✅ Model configurat dimensiune 768\n",
      "🔄 Generez embeddings pentru 4558 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [10:48<00:00,  4.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 4558 embeddings generate\n",
      "✅ Index FAISS creat cu 4558 vectori de dimensiune 768\n",
      "💾 Vector DB assets salvate în processed:\n",
      "   📁 embeddings_with_metadata.json - pentru vector DB\n",
      "   📁 faiss_index.bin - pentru căutare\n",
      "   📁 embeddings.npy - array numpy\n",
      "   📁 chunks_text.json - texte pentru debug\n",
      "\n",
      "🔍 Demo căutare:\n",
      "\n",
      "❓ Întrebare: 'contractul de muncă'\n",
      "   1. [CM_art_37] Score: 0.574\n",
      "      si obligatiile privind relatiile de munca dintre angajator si salariat se stabilesc potrivit legii, ...\n",
      "   2. [CM_art_21] Score: 0.536\n",
      "      La incheierea contractului individual de munca sau pe parcursul executarii acestuia, partile pot neg...\n",
      "   3. [CM_art_16] Score: 0.525\n",
      "      Contractul individual de munca se incheie in baza consimtamantului partilor, in forma scrisa, in lim...\n",
      "\n",
      "❓ Întrebare: 'drepturile salariatului'\n",
      "   1. [CM_art_39] Score: 0.738\n",
      "      Salariatul are, in principal, urmatoarele drepturi: \n",
      "a) dreptul la salarizare pentru munca depusa; \n",
      "...\n",
      "   2. [CM_art_47] Score: 0.714\n",
      "      Drepturile cuvenite salariatului detasat se acorda de angajatorul la care s-a dispus detasarea. \n",
      "(2)...\n",
      "   3. [CONST_art_43] Score: 0.615\n",
      "      Salariaţii au dreptul la grevă pentru apărarea intereselor profesionale, economice\n",
      "şi sociale.\n",
      "(2) L...\n",
      "\n",
      "❓ Întrebare: 'concediul de odihnă'\n",
      "   1. [CM_art_133] Score: 0.719\n",
      "      de repaus reprezinta orice perioada care nu este timp de munca. \n",
      "SECTIUNEA 1 \n",
      "Pauza de masa si repau...\n",
      "   2. [CM_art_151] Score: 0.360\n",
      "      Concediul de odihna poate fi intrerupt, la cererea salariatului, pentru motive obiective. \n",
      "(2) Angaj...\n",
      "   3. [CM_art_149] Score: 0.314\n",
      "      este obligat sa efectueze in natura concediul de odihna in perioada in care a fost programat, cu exc...\n",
      "\n",
      "❓ Întrebare: 'salariul minim'\n",
      "   1. [CM_art_164] Score: 0.538\n",
      "      Salariul de baza minim brut pe tara garantat in plata, corespunzator programului normal de munca, se...\n",
      "   2. [CM_art_165] Score: 0.466\n",
      "      salariatii carora angajatorul, conform contractului colectiv sau individual de munca, le asigura hra...\n",
      "   3. [CM_art_103] Score: 0.415\n",
      "      cu fractiune de norma este salariatul al carui numar de ore normale de lucru, calculate saptamanal s...\n",
      "\n",
      "❓ Întrebare: 'întreruperea contractului de muncă'\n",
      "   1. [CM_art_21] Score: 0.513\n",
      "      La incheierea contractului individual de munca sau pe parcursul executarii acestuia, partile pot neg...\n",
      "   2. [CM_art_56] Score: 0.510\n",
      "      Contractul individual de munca existent inceteaza de drept: \n",
      "a) la data decesului salariatului sau a...\n",
      "   3. [CM_art_22] Score: 0.458\n",
      "      Clauza de neconcurenta isi poate produce efectele pentru o perioada de maximum 2 ani de la data ince...\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'Pagina?\\s+\\d+\\s+din\\s+\\d+', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\d+/\\d+', '', text)\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    text = re.sub(r'\\d{2}\\.\\d{2}\\.\\d{4}', '', text)\n",
    "    \n",
    "    # Normalizează spațiile\n",
    "    text = re.sub(r'\\s{3,}', ' ', text)\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def identify_doc_type(filename):\n",
    "    filename = filename.lower()\n",
    "    if 'codul-muncii' in filename or 'cod muncii' in filename:\n",
    "        return 'CM'\n",
    "    elif 'constitutia' in filename:\n",
    "        return 'CONST'\n",
    "    elif 'cod-civil' in filename or 'cod civil' in filename:\n",
    "        return 'CC'\n",
    "    elif 'cod-fiscal' in filename or 'cod fiscal' in filename:\n",
    "        return 'CF'\n",
    "    else:\n",
    "        return 'LEGAL'\n",
    "\n",
    "def is_article_reference(text_line):\n",
    "\n",
    "    reference_patterns = [\n",
    "        r'^\\s*(?:conform|potrivit|prevederile|prevazut\\s+(?:la|de)|dispozitiile|in\\s+sensul|in\\s+conditiile)\\s+(?:art\\.|articolul)\\s+\\d+',\n",
    "        r'(?:conform|potrivit|prevederile|prevazut\\s+(?:la|de)|dispozitiile)\\s+(?:art\\.|articolul)\\s+\\d+(?:\\s*[-,.]|\\s*$)',\n",
    "        r'^\\s*(?:art\\.|articolul)\\s+\\d+\\s*[-,.]?\\s*(?:alin|lit|pct)\\.',  # art. 132 alin. 1\n",
    "        r'la\\s+(?:art\\.|articolul)\\s+\\d+',  # la art. 132\n",
    "        r'din\\s+(?:art\\.|articolul)\\s+\\d+', # din art. 132\n",
    "        r'de\\s+(?:art\\.|articolul)\\s+\\d+'\n",
    "    ]\n",
    "\n",
    "    text_lower = text_line.lower()\n",
    "    for pattern in reference_patterns:\n",
    "        if re.search(pattern, text_lower):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def extract_articles(text, doc_type):\n",
    "    print(f\"🔍 DEBUG pentru {doc_type}:\")\n",
    "    chunks = []\n",
    "    \n",
    "    ARTICLE_RE = re.compile(\n",
    "        r'(?:^|\\s)(?:ARTICOLUL|Articolul|ART\\.?|Art\\.)\\s+(\\d+(?:\\^\\d+)?)\\.?\\b'\n",
    "        r'(?:\\s*[-–]\\s*)?(?:\\s*\\n?\\s*(.+?))?(?=\\s|$|\\n)',\n",
    "        flags=re.MULTILINE\n",
    "    )\n",
    "    \n",
    "    ARTICLE_REFERENCE = re.compile(\n",
    "        r'(?:conform|potrivit|prevederile|prevazut\\s+(?:la|de)|dispozitiile|la|din)\\s+(?:art\\.|articolul)\\s+\\d+\\b',\n",
    "        flags=re.MULTILINE | re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    all_matches = ARTICLE_RE.findall(text)\n",
    "    print(f\"   Total articole găsite cu ARTICLE_RE: {len(all_matches)}\")\n",
    "    if all_matches:\n",
    "        print(f\"   Primele 10: {all_matches[:10]}\")\n",
    "    \n",
    "    # Gasește referințele pentru a le exclude ulterior\n",
    "    references = ARTICLE_REFERENCE.findall(text)\n",
    "    print(f\"   Referințe găsite (de exclus): {len(references)}\")\n",
    "    if references:\n",
    "        print(f\"   Primele referințe: {references[:5]}\")\n",
    "    \n",
    "    # Iterează prin toate meciurile găsite\n",
    "    for match in ARTICLE_RE.finditer(text):\n",
    "        full_match = match.group(0)\n",
    "        article_num = match.group(1)\n",
    "        article_title = match.group(2) if match.group(2) else \"\"\n",
    "        \n",
    "        #print(f\"   🔍 Găsit: '{full_match.strip()}'\")\n",
    "        \n",
    "        # Verificare mai riguroasă pentru referințe\n",
    "        context_start = max(0, match.start() - 100)  # Context mai mare\n",
    "        context_end = min(len(text), match.end() + 50)\n",
    "        context = text[context_start:context_end]\n",
    "        \n",
    "        # Verificăm dacă articolul este într-un context de referință\n",
    "        is_reference = False\n",
    "        \n",
    "        # 1. Verificăm contextul din jurul match-ului\n",
    "        if ARTICLE_REFERENCE.search(context):\n",
    "            is_reference = True\n",
    "            \n",
    "        # 2. Verificăm dacă linia întreagă pare a fi o referință\n",
    "        line_start = text.rfind('\\n', 0, match.start()) + 1\n",
    "        line_end = text.find('\\n', match.end())\n",
    "        if line_end == -1:\n",
    "            line_end = len(text)\n",
    "        full_line = text[line_start:line_end]\n",
    "        \n",
    "        if is_article_reference(full_line):\n",
    "            is_reference = True\n",
    "            \n",
    "        # 3. Verificăm dacă nu urmează conținut substanțial\n",
    "        content_preview = text[match.end():match.end()+10].strip()\n",
    "        if len(content_preview) < 10 and re.search(r'(?:art\\.|articolul)\\s+\\d+', content_preview, re.IGNORECASE):\n",
    "            is_reference = True\n",
    "        \n",
    "        if not is_reference:\n",
    "            # Extragem conținutul articolului\n",
    "            start_pos = match.end()\n",
    "            \n",
    "            # Găsim sfârșitul articolului (următorul articol sau sfârșitul textului)\n",
    "            next_article = ARTICLE_RE.search(text, start_pos)\n",
    "            if next_article:\n",
    "                end_pos = next_article.start()\n",
    "                content = text[start_pos:end_pos].strip()\n",
    "            else:\n",
    "                content = text[start_pos:].strip()\n",
    "            \n",
    "            # Curățăm conținutul de linii goale multiple\n",
    "            content = re.sub(r'\\n{3,}', '\\n\\n', content)\n",
    "            content = content.strip()\n",
    "            \n",
    "            if content and len(content) > 10:  # Doar dacă avem conținut substanțial\n",
    "                chunk = {\n",
    "                    'id': f\"{doc_type}_art_{article_num}\",\n",
    "                    'text': content,\n",
    "                    'metadata': {\n",
    "                        'doc_type': doc_type,\n",
    "                        'article_number': article_num,\n",
    "                        'article_title': article_title.strip() if article_title else \"\"\n",
    "                    }\n",
    "                }\n",
    "                chunks.append(chunk)\n",
    "                #print(f\"   ✅ Articol {article_num} extras ({len(content)} caractere)\")\n",
    "            else:\n",
    "                print(f\"   ⚠️  Articol {article_num} - conținut prea scurt: '{content[:50]}'\")\n",
    "        #else:\n",
    "            #print(f\"   ✗ Exclus ca referință: 'Art. {article_num}'\")\n",
    "            #print(f\"      Context: '{context[:20]}...'\")\n",
    "            #print(f\"      Linie completă: '{full_line[:20]}...'\")\n",
    "    \n",
    "    print(f\"   🎯 FINAL: {len(chunks)} articole extrase din {doc_type}\")\n",
    "    return chunks\n",
    "\n",
    "def process_pdf(pdf_path):\n",
    "\n",
    "    print(f\"🔍 Procesez: {pdf_path.name}\")\n",
    "    \n",
    "    raw_text = extract_text_from_pdf(pdf_path)\n",
    "    print(f\"   Lungime text brut: {len(raw_text)} caractere\")\n",
    "    \n",
    "    clean_text_content = clean_text(raw_text)\n",
    "    print(f\"   Lungime text curat: {len(clean_text_content)} caractere\")\n",
    "    \n",
    "    doc_type = identify_doc_type(pdf_path.name)\n",
    "    print(f\"   Tip document: {doc_type}\")\n",
    "    \n",
    "    chunks = extract_articles(clean_text_content, doc_type)\n",
    "    print(f\"   ✅ {len(chunks)} articole extrase\")\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def setup_embeddings():\n",
    "    print(\"🔄 Configurare model embeddings...\")\n",
    "\n",
    "    model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "    test_embedding = model.encode([\"test\"], convert_to_numpy=True)\n",
    "    print(f\"✅ Model configurat dimensiune {test_embedding.shape[1]}\")\n",
    "    return model\n",
    "\n",
    "def generate_embeddings(chunks, model, batch_size=32):    \n",
    "    print(f\"🔄 Generez embeddings pentru {len(chunks)} chunks...\")\n",
    "\n",
    "    texts = [chunk['text'] for chunk in chunks]\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(chunks), batch_size)):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        batch_embeddings = model.encode(batch, convert_to_numpy=True, show_progress_bar=False)\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    for chunk, embedding in zip(chunks, embeddings):\n",
    "        chunk['embedding'] = embedding\n",
    "    \n",
    "    print(f\"✅ {len(embeddings)} embeddings generate\")\n",
    "    return chunks\n",
    "\n",
    "def create_faiss_index(chunks_with_embeddings):\n",
    "\n",
    "    embeddings = np.array([chunk['embedding'] for chunk in chunks_with_embeddings])\n",
    "    dimension = embeddings.shape[1]\n",
    "    \n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    print(f\"✅ Index FAISS creat cu {index.ntotal} vectori de dimensiune {dimension}\")\n",
    "    return index\n",
    "\n",
    "def save_processed_data(chunks, index, model, output_dir=\"processed\"):\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # 1.  Save embeddings + metadata pentru vector DB\n",
    "    embeddings_data = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        embeddings_data.append({\n",
    "            'id': chunk['id'],\n",
    "            'embedding': chunk['embedding'].tolist(),  # Convert numpy to list for JSON\n",
    "            'metadata': chunk['metadata']\n",
    "        })\n",
    "    \n",
    "    with open(output_path / \"embeddings_with_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(embeddings_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Debbug: Salvează textul brut al chunk-urilor\n",
    "    texts_data = []\n",
    "    for chunk in chunks:\n",
    "        texts_data.append({\n",
    "            'id': chunk['id'],\n",
    "            'text': chunk['text'],\n",
    "            'metadata': chunk['metadata']\n",
    "        })\n",
    "    \n",
    "    with open(output_path / \"chunks_text.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(texts_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # 3. Salvam index FAISS - necesar pentru căutare rapidă\n",
    "    faiss.write_index(index, str(output_path / \"faiss_index.bin\"))\n",
    "    \n",
    "    # 4. Salvam embeddings ca numpy array - loading rapid \n",
    "    embeddings_array = np.array([chunk['embedding'] for chunk in chunks])\n",
    "    np.save(output_path / \"embeddings.npy\", embeddings_array)\n",
    "    \n",
    "    print(f\"💾 Vector DB assets salvate în {output_dir}:\")\n",
    "    print(f\"   📁 embeddings_with_metadata.json - pentru vector DB\")\n",
    "    print(f\"   📁 faiss_index.bin - pentru căutare\")\n",
    "    print(f\"   📁 embeddings.npy - array numpy\")\n",
    "    print(f\"   📁 chunks_text.json - texte pentru debug\")\n",
    "\n",
    "def load_processed_data(input_dir=\"processed\"):\n",
    "\n",
    "    input_path = Path(input_dir)\n",
    "    \n",
    "    with open(input_path / \"embeddings_with_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        embeddings_data = json.load(f)\n",
    "    \n",
    "    index = faiss.read_index(str(input_path / \"faiss_index.bin\"))\n",
    "    \n",
    "    with open(input_path / \"chunks_text.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        texts_data = json.load(f)\n",
    "    \n",
    "    print(f\"✅ Date încărcate din {input_dir}\")\n",
    "    print(f\"   📊 {len(embeddings_data)} embeddings\")\n",
    "    print(f\"   🔍 Index FAISS cu {index.ntotal} vectori\")\n",
    "    \n",
    "    return embeddings_data, index, texts_data\n",
    "\n",
    "def show_statistics(chunks):\n",
    "    print(\"\\n📊 Statistici chunks:\")\n",
    "    print(f\"Total chunks: {len(chunks)}\")\n",
    "    \n",
    "    doc_stats = {}\n",
    "    for chunk in chunks:\n",
    "        doc_type = chunk['metadata']['doc_type']\n",
    "        doc_stats[doc_type] = doc_stats.get(doc_type, 0) + 1\n",
    "    \n",
    "    for doc_type, count in sorted(doc_stats.items()):\n",
    "        print(f\"{doc_type}: {count} articole\")\n",
    "\n",
    "def show_failed_extractions(directory_path, chunks):\n",
    "\n",
    "    directory = Path(directory_path)\n",
    "    pdf_files = list(directory.glob(\"*.pdf\"))\n",
    "    \n",
    "    # Colectează tipurile de documente care au avut extracții\n",
    "    extracted_files = {}\n",
    "    for chunk in chunks:\n",
    "        doc_type = chunk['metadata']['doc_type']\n",
    "        if doc_type not in extracted_files:\n",
    "            extracted_files[doc_type] = []\n",
    "        extracted_files[doc_type].append(chunk)\n",
    "    \n",
    "    failed_files = []\n",
    "    \n",
    "    print(\"\\n❌ Analiză fișiere:\")\n",
    "    for pdf_file in pdf_files:\n",
    "        doc_type = identify_doc_type(pdf_file.name)\n",
    "        chunk_count = len(extracted_files.get(doc_type, []))\n",
    "        \n",
    "        if chunk_count == 0:\n",
    "            failed_files.append(pdf_file)\n",
    "            print(f\"   ❌ {pdf_file.name} (tip: {doc_type}) - 0 articole\")\n",
    "        else:\n",
    "            print(f\"   ✅ {pdf_file.name} (tip: {doc_type}) - {chunk_count} articole\")\n",
    "    \n",
    "    # Pentru fișierele failed, arată formatul textului\n",
    "    if failed_files:\n",
    "        print(f\"\\n🔍 ANALIZA FORMATELOR pentru {len(failed_files)} fișiere failed:\")\n",
    "        \n",
    "        for pdf_file in failed_files:\n",
    "            print(f\"\\n📄 === {pdf_file.name} ===\")\n",
    "            try:\n",
    "                raw_text = extract_text_from_pdf(pdf_file)\n",
    "                clean_text_content = clean_text(raw_text)\n",
    "                                \n",
    "                # Caută toate pattern-urile posibile\n",
    "                patterns_to_check = [\n",
    "                    (r'ARTICOLUL\\s+\\d+', 'ARTICOLUL num'),\n",
    "                    (r'Articolul\\s+\\d+', 'Articolul num'),\n",
    "                    (r'Art\\.\\s+\\d+', 'Art. num'),\n",
    "                    (r'ART\\.\\s+\\d+', 'ART. num'),\n",
    "                    (r'art\\.\\s+\\d+', 'art. num'),\n",
    "                    (r'Art\\s+\\d+', 'Art num (fără punct)'),\n",
    "                    (r'ART\\s+\\d+', 'ART num (fără punct)'),\n",
    "                    (r'Article\\s+\\d+', 'Article num (engleza)'),\n",
    "                    (r'\\d+\\.\\s+', 'num. (format simplu)'),\n",
    "                ]\n",
    "                \n",
    "                print(f\"🔍 Pattern-uri găsite:\")\n",
    "                found_any = False\n",
    "                for pattern, description in patterns_to_check:\n",
    "                    matches = re.findall(pattern, clean_text_content, re.MULTILINE | re.IGNORECASE)\n",
    "                    if matches:\n",
    "                        print(f\"   ✓ {description}: {len(matches)} găsite - {matches[:5]}\")\n",
    "                        found_any = True\n",
    "                \n",
    "                if not found_any:\n",
    "                    print(\"   ❌ Niciun pattern de articol găsit!\")\n",
    "                \n",
    "                # Caută linii care încep cu numere\n",
    "                lines_with_numbers = []\n",
    "                for line_num, line in enumerate(clean_text_content.split('\\n')[:100]):\n",
    "                    if re.match(r'^\\s*\\d+', line.strip()):\n",
    "                        lines_with_numbers.append(f\"Linia {line_num}: '{line.strip()}'\")\n",
    "                \n",
    "                if lines_with_numbers:\n",
    "                    print(f\"📊 Primele linii care încep cu numere:\")\n",
    "                    for line_info in lines_with_numbers[:10]:\n",
    "                        print(f\"   {line_info}\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Eroare la procesarea {pdf_file.name}: {e}\")\n",
    "    else:\n",
    "        print(\"✅ Toate fișierele au fost procesate cu succes!\")\n",
    "\n",
    "def search_chunks(query, chunks, index, model, top_k=5):\n",
    "    \"\"\"Caută în chunks folosind similaritatea semantică\"\"\"\n",
    "\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    results = []\n",
    "    for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "        if idx < len(chunks):\n",
    "            results.append({\n",
    "                'rank': i + 1,\n",
    "                'score': 1 - distance,  \n",
    "                'chunk': chunks[idx]\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def demo_search(chunks, index, model):\n",
    "\n",
    "    test_queries = [\n",
    "        \"contractul de muncă\",\n",
    "        \"drepturile salariatului\",\n",
    "        \"concediul de odihnă\",\n",
    "        \"salariul minim\",\n",
    "        \"întreruperea contractului de muncă\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n🔍 Demo căutare:\")\n",
    "    for query in test_queries:\n",
    "        print(f\"\\n❓ Întrebare: '{query}'\")\n",
    "        results = search_chunks(query, chunks, index, model, top_k=3)\n",
    "        \n",
    "        for result in results:\n",
    "            chunk = result['chunk']\n",
    "            print(f\"   {result['rank']}. [{chunk['id']}] Score: {result['score']:.3f}\")\n",
    "            print(f\"      {chunk['text'][:100]}...\")\n",
    "\n",
    "def process_directory(directory_path):\n",
    "    print(f\"🗂️  Procesez directorul: {directory_path}\")\n",
    "\n",
    "    directory = Path(directory_path)\n",
    "    pdf_files = list(directory.glob(\"*.pdf\"))\n",
    "    all_chunks = []\n",
    "\n",
    "    print(f\"   Găsite {len(pdf_files)} fișiere PDF\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        chunks = process_pdf(pdf_file)\n",
    "        all_chunks.extend(chunks)\n",
    "    \n",
    "    print(f\"\\n✅ Total chunks create: {len(all_chunks)}\")\n",
    "    return all_chunks\n",
    "\n",
    "def main():\n",
    "    directory_path = \"dataset\"\n",
    "    \n",
    "    chunks = process_directory(directory_path)\n",
    "    show_statistics(chunks)\n",
    "    show_failed_extractions(directory_path, chunks)\n",
    "    \n",
    "    model = setup_embeddings()    \n",
    "    chunks_with_embeddings = generate_embeddings(chunks, model)\n",
    "    index = create_faiss_index(chunks_with_embeddings)\n",
    "    \n",
    "    save_processed_data(chunks_with_embeddings, index, model)\n",
    "    demo_search(chunks_with_embeddings, index, model)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
